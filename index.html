<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Title -->
    <title>Xiaojian Ma's Homepage</title>

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css"
        integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
    <!-- https://fontawesome.com/cheatsheet -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css"
        integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-124898353-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'UA-124898353-1');
    </script>

    <!-- Add Bootstrap JS and dependencies -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.4/dist/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>


</head>

<body>
    <nav class="navbar navbar-expand-md navbar-dark fixed-top" style="background-color: #122C56;">

        <a class="navbar-brand" href="#">Xiaojian Ma</a>

        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarToggle">
            <ul class="navbar-nav ml-auto">
                <li class="nav-item">
                    <a class="nav-link" href="#">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Publication">Publication</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Experience">Experience</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Contact">Contact</a>
                </li>
            </ul>
        </div>
    </nav>

    <div class="container" style="padding-top: 20px; padding-bottom: 10px; font-size: 19px">
        <div class="row">
            <div class="col-md-3" , style="padding-right: 0px; padding-top: 120px">
                <img class="img-responsive img-rounded" src="img/xiaojian_23.jpg" alt=""
                    style="max-width: 260px; border:4px solid gray">
            </div>
            <div class="col-md-9" , style="padding-left: 50px; padding-right: 0px; padding-top: 40px; font-size: 16px">

                <br>
                <p style="padding-top: 10px;">Welcome to my academic homepage. I am a machine learning researcher. I received my Ph.D. in
                    <a target="_blank" href="http://cs.ucla.edu/">
                        <font color="black">Computer Science at University of California, Los Angeles (UCLA)</font>
                    </a>. I went to <a target="_blank" href="https://www.tsinghua.edu.cn/en/">
                        <font color="black">Tsinghua University</font>
                    </a> for undergraduate study in Computer Science.

                <p style="padding-bottom: 12px;">
                    I work on multimodal learning for understanding, reasoning and skill learning. In particular, I'm
                    interested in building models/agents that can learn from 2D/3D vision and text data, and perform a
                    wide range of reasoning and embodied control tasks. Some of my research keywords can be found below:
                <ul compact>
                    <li><b>Multimodal learning</b>: Vision and language, Visual reasoning, 3D vision, Generalist models
                    </li>
                    <li><b>Representation learning</b>: Zero-shot and few-shot learning, Generative model</li>
                    <li><b>Embodied agents</b>: Reinforcement learning and imitation, Robotics, Sensor fusion</li>
                </ul>

                <div style="font-size: 16px;">
                    <b>Email</b>: jeasinema [at] gmail [dot] com / <a
                        href="https://scholar.google.com/citations?user=xZec9goAAAAJ">Google Scholar</a> / <a
                        href="https://www.linkedin.com/in/xiaojian-m-880096139/">LinkedIn</a>
                </div>
                <p style="padding-bottom: 10px;">
                    <a href="https://twitter.com/jeasinema?ref_src=twsrc%5Etfw" class="twitter-follow-button"
                        data-show-count="false">Follow @jeasinema</a>
                    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
            </div>
        </div>
    </div>

    <!-- News -->
    <div class="container">
        <h3 id="News" style="padding-top: 80px; margin-top: -80px;">News</h3>
        <hr>

        <ul>
            <li style="font-size: 16px">
                <b>[01/2025]</b> Some recent efforts on perception for embodied agents (long-form videos, dynamic scenes): <a target='_blank'
                    href='https://embodied-videoagent.github.io/'>
                    <font color='black'><b>Embodied VideoAgent</font></b>
                </a>, <a target='_blank' href='https://rujiewu.github.io/LongViTU.github.io/'>
                    <font color='black'><b>LongViTU</font></b>
                </a>
            </li>
            <li style="font-size: 16px">
                <b>[12/2024]</b> Some recent efforts on multi-modal, open-world agents: <a target='_blank'
                    href='https://craftjarvis.github.io/ROCKET-1/'>
                    <font color='black'><b>ROCKET-1</font></b>
                </a>, <a target='_blank' href='https://mat-agent.github.io/'>
                    <font color='black'><b>Multi-modal Agent Tuning</font></b>
                </a>
            </li>
            <li style="font-size: 16px">
                <b>[10/2024]</b> We will be hosting <a target='_blank' href='https://owa-workshop.github.io'><font color='black'><b>NeurIPS 2024 Workshop on Open-World Agents</b></font></a>. Come join us in Vancouver, BC, Canada this winter! 
            </li>
            <li style="font-size: 16px">
                <b>[07/2024]</b> Still stuck on InstructPix2Pix? Try our <a target='_blank' href='https://huggingface.co/spaces/jeasinema/UltraEdit-SD3'><font color='black'><b>UltraEdit</b></font></a> for free-form and region-based image editing with language.
            </li>
            <li style="font-size: 16px">
                <b>[06/2024]</b> New vision-language-action model (VLA) exploring native interaction data modeling of embodied agents: <a target='_blank' href='http://omnijarvis.github.io'><font color='black'><b>OmniJARVIS</font></b></a>.
            </li>

            <!--li style="font-size: 16px">
                <b>[03/2024]</b> Understanding long-form video is essential to general embodied and interactive agents. Checkout <a target='_blank' href='http://videoagent.github.io'><font color='black'><b>VideoAgent</font></b></a>, a zero-shot memory-based agent for video understanding. More is coming next.
            </li-->

            <!--li style="font-size: 16px">
                <b>[01/2024]</b> <a target='_blank' href='http://craftjarvis.github.io/GROOT'>
                    <font color='black'><b>GROOT</font></b>
                </a>, a large-scale instruction-following agent, is accepted to <b>ICLR 2024</b> with a <b>spotlight</b> presentation.
            </li-->

            <!--li style="font-size: 16px">
                <b>[11/2023]</b> Some recent efforts on large-scale embodied agents: <a target='_blank'
                    href='https://embodied-generalist.github.io/'>
                    <font color='black'><b>LEO</font></b>
                </a>, <a target='_blank' href='https://craftjarvis-jarvis1.github.io/'>
                    <font color='black'><b>JARVIS-1</font></b>
                </a> and <a target='_blank' href='https://craftjarvis-groot.github.io/'>
                    <font color='black'><b>GROOT</font></b>
                </a> (<a target='_blank' href='file/leo_n_craftjarvis_11_2023.pdf'>slides</a>).
            </li -->
            <!--li style="font-size: 16px">
                <b>[07/2023]</b> <a target='_blank' href='https://3d-vista.github.io/'>
                    <font color='black'><b>3D-VisTA</font></b>
                </a>, a 3D-Language foundation model, is accepted to <b>ICCV 2023</b>.
            </li-->
            <!-- <li style="font-size: 16px">
                <b>[07/2023]</b> <a target='_blank' href='https://arxiv.org/abs/2302.01560'>
                    <font color='black'><b>DEPS</font></b>
                </a> has won the best paper award at ICML-23 TEACH Workshop. Congratulations to the team!
            </li>
            -->
            <!-- <li style="font-size: 16px">
                <b>[04/2023]</b> I defended my Ph.D. thesis "A Unified Framework with Benchmarks for Human-like Visual
                and Relational Reasoning in the Real World". Thanks everyone has supported me along this journey.
            </li> -->
            <!-- <li style="font-size: 16px">
                <b>[02/2023]</b> Two new preprints on generalist open-world <a target='_blank' href='https://arxiv.org/abs/2302.01560'><font color='black'><b>planning</font></b></a> and <a target='_blank' href='https://arxiv.org/abs/2301.10034'><font color='black'><b>control</font></b></a> in Minecraft are available.
            </li>
            <li style="font-size: 16px">
                <b>[01/2023]</b> One paper on embodied 3D scene understanding is accepted to <a target='_blank' href='https://iclr.cc/'><font color='black'><b>ICLR 2023</font></b></a>. <a target='_blank' href='https://github.com/SilongYong/SQA3D'>Code</a> has been released. Check out live examples at our <a target='_blank' href='https://sqa3d.github.io/'>Project site</a>.
            </li> -->
        </ul>
    </div>

    <!-- Publications -->
    <div class="container">
        <h3 id="Publication" style="padding-top: 80px; margin-top: -80px;">Selected Publications</h3>
        <hr>

        <h4 id="Preprint" style="padding-top: 40px; margin-top: -40px;">Preprint</h4>

        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/evideoagent_arxiv25.gif" alt="">
            </div>
            <div class="col-md-9">
                Yue Fan*,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma*</b></a>,
                Rongpeng Su,
                Jun Guo,
                <a href='https://joyjayng.github.io/'><b>Rujie Wu</b></a>,
                Xi Chen,
                <a href="https://liqing-ustc.github.io">Qing Li</a>
                <br>
                <b>
                    <font color="black">Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding</font>
                </b><br>
                <a href="#"><b>arXiv preprint</b></a>&nbsp;/
                <a target="_blank" href="https://www.arxiv.org/abs/2501.00358">arXiv</a>&nbsp;/
                <a target="_blank" href="https://embodied-videoagent.github.io/">Project</a>
                <br>
                <font color="firebrick"> A memory framework for embodied agents in household environments (and beyond!)</font>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/longvitu_arxiv25.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://joyjayng.github.io/'><b>Rujie Wu*</b></a>,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma*</b></a>,
                Hai Ci,
                Yue Fan,
                Rongpeng Su,
                Yuxuan Wang,
                Haozhe Zhao,
                <a href="https://liqing-ustc.github.io">Qing Li</a>,
                Yizhou Wang,
                <br>
                <b>
                    <font color="black">LongViTU: Instruction Tuning for Long-Form Video Understanding</font>
                </b><br>
                <a href="#"><b>arXiv preprint</b></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2501.05037">arXiv</a>&nbsp;/
                <a target="_blank" href="https://rujiewu.github.io/LongViTU.github.io/">Project</a>
                <br>
                <font color="firebrick"> Synthetic data for long-form video understanding.</font>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/semanticgs_arxiv24.gif" alt="">
            </div>
            <div class="col-md-9">
                Jun Guo*,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma*</b></a>,
                Yue Fan,
                <a href="https://sites.google.com/site/thuliuhuaping/home">Huaping Liu</a>,
                <a href="https://liqing-ustc.github.io">Qing Li</a>
                <br>
                <b>
                    <font color="black">Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian Splatting</font>
                </b><br>
                <a href="#"><b>arXiv preprint</b></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2403.15624">arXiv</a>&nbsp;/
                <a target="_blank" href="https://semantic-gaussians.github.io/">Project</a>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/rat_arxiv24.png" alt="">
            </div>
            <div class="col-md-9">
                <a href="https://craftjarvis.org/">Team CraftJarvis <img src="img/craftjarvis1.png" alt="Icon" style="width: 100px; height: 25px; vertical-align: -5px;"></a>
                <br>
                <b>
                    <font color="black">üê≠ RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation</font>
                </b><br>
                <a href="#"><b>arXiv preprint</b></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2403.05313">arXiv</a>&nbsp;/
                <a target="_blank" href="https://craftjarvis.github.io/RAT/">Project</a>&nbsp;/
                <a target="_blank" href="https://huggingface.co/spaces/jeasinema/RAT">Demo</a>&nbsp;
                <br>
                <font color="firebrick"> Significant improvement on creativity writing, codegen, and math problems.
                </font>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/halma_arxiv21.jpg" alt="">
            </div>
            <div class="col-md-9">
                <a href='http://siruixie.com'>Sirui Xie</a>,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                Peiyu Yu,
                <a href="https://yzhu.io">Yixin Zhu</a>,
                <a href="http://www.stat.ucla.edu/~ywu">Ying Nian Wu</a> and
                <a href="http://www.stat.ucla.edu/~sczhu">Song-Chun Zhu</a>
                <br>
                <b>
                    <font color="black">HALMA: Humanlike Abstraction Learning Meets Affordance in Rapid Problem Solving
                    </font>
                </b><br>
                <a href="https://arxiv.org/abs/2102.11344"><b>arXiv preprint</b></a>&nbsp;/
                <a href="file/halma_arxiv21.pdf">Paper</a>&nbsp;/
                <a target="_blank" href="https://halma-proj.github.io"><b>
                        <font color="firebrick">Click Here to Play HALMA!</font>
                    </b></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2102.11344">arXiv</a>&nbsp;
            </div>
        </div>
        <br>

        <!--div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/cdmp_arxiv18.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>*,
                Mingxuan Jing*,
                Fuchun Sun and
                <a href="https://sites.google.com/site/thuliuhuaping/home">Huaping Liu</a>
                <br>
                <b><font color="black">Learning and Inferring Movement from Demonstrations with Deep Generative Model</font></b><br>
                <a href="https://arxiv.org/abs/1805.07252"><b>arXiv preprint</b></a>&nbsp;/
                    <a target="_blank" href="file/cdmp_arxiv18.pdf">Paper</a>&nbsp;/
                    Project Page&nbsp;/
                    <a target="_blank" href="https://arxiv.org/abs/1805.07252">arXiv</a>&nbsp;/
                    <a target="_blank" href="https://github.com/tsinghua-rll/CDMP">Code</a>&nbsp;
                <br>
                also in <a target="_blank" href="https://sites.google.com/view/mbrl-icml2019/home">Generative Modeling and Model-Based Reasoning for Robotics and AI Workshop @ <b>ICML 2019</b></a><br>
            </div>
        </div-->
        <br>

        <h4 id="Conferences" style="padding-top: 40px; margin-top: -40px;">Conference and Journal</h4>

        <!-- CVPR-25-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/rocket1_arxiv24.gif" alt="">
            </div>
            <div class="col-md-9">
                <a href="https://craftjarvis.org/">Team CraftJarvis <img src="img/craftjarvis1.png" alt="Icon" style="width: 100px; height: 25px; vertical-align: -5px;"></a>
                <br>
                <b>
                    <font color="black">ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting</font>
                </b><br>
                <a href="https://cvpr.thecvf.com/Conferences/2025/"><b>CVPR 2025</b></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2410.17856">arXiv</a>&nbsp;/
                <a target="_blank" href="https://craftjarvis.github.io/ROCKET-1/">Project</a>
                <br>
                <font color="firebrick"> Learning to interact with all your surroundings, from self-supervision, enables open-world agents :) </font>
            </div>
        </div>
        <br>

        <!-- ICLR-25-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/mat_arxiv24.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://zhigao2017.github.io/'>Zhi Gao*</a>,
                <a href='https://bofei5675.github.io/'>Bofei Zhang*</a>,
                <a href='https://pengxiang-li.github.io/'>Pengxiang Li*</a>,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                Yue Fan,
                Tao Yuan,
                Yuwei Wu,
                Yunde Jia,
                Song-Chun Zhu,
                <a href="https://liqing-ustc.github.io">Qing Li</a>,
                <br>
                <b>
                    <font color="black">Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage</font>
                </b><br>
                <a target="_blank" href="https://iclr.cc/Conferences/2025"><b>ICLR 2025</b></font></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/pdf/2412.15606">arXiv</a>&nbsp;/
                <a target="_blank" href="https://mat-agent.github.io/">Project</a>
                <br>
                <font color="firebrick"> Spotlight presentation. </font>

            </div>
        </div>
        <br>

        <!-- T-PAMI-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/jarvis1_arxiv23.png" alt="">
            </div>
            <div class="col-md-9">
                <a href="https://craftjarvis.org/">Team CraftJarvis <img src="img/craftjarvis1.png" alt="Icon" style="width: 100px; height: 25px; vertical-align: -5px;"></a>
                <br>
                <b>
                    <font color="black">JARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language
                        Models</font>
                </b><br>
                <a href="https://www.computer.org/csdl/journal/tp"><b>T-PAMI</b></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2311.05997">arXiv</a>&nbsp;/
                <a target="_blank" href="https://craftjarvis-jarvis1.github.io/">Project</a>&nbsp;/
                <a target="_blank" href="https://github.com/CraftJarvis/JARVIS-1">Code</a>&nbsp;
                <br>
                <font color="firebrick"> Embodied RAG meets open-world agents.
                </font>
            </div>
        </div>
        <br>

        <!-- NeurIPS-24-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/ultraedit_arxiv24.png" alt="">
            </div>
            <div class="col-md-9">
                <a href="https://scholar.google.cz/citations?user=skIXywUAAAAJ&hl=zh-CN">Haozhe Zhao*</a>,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma*</b></a>,
                Liang Chen,
                Shuzheng Si,
                Rujie Wu
                Kaikai An,
                Peiyu Yu,
                <a href="https://minjiazhang.github.io/">Minjia Zhang</a>,
                <a href="https://liqing-ustc.github.io">Qing Li</a>
                <a href="https://scholar.google.com.au/citations?user=LaKNyhQAAAAJ&hl=en">Baobao Chang</a>
                <br>
                <b>
                    <font color="black">UltraEdit: Instruction-based Fine-Grained Image Editing at Scale</font>
                </b><br>
                <a target="_blank" href="https://nips.cc/Conferences/2024"><b>NeurIPS 2024 D&B Track</b></font></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2407.05282">arXiv</a>&nbsp;/
                <a target="_blank" href="https://huggingface.co/spaces/jeasinema/UltraEdit-SD3">Demo</a>&nbsp;/
                <a target="_blank" href="https://ultra-editing.github.io/">Project</a>&nbsp;/
                <a target="_blank" href="https://github.com/HaozheZhao/UltraEdit">Code</a>
                <br>
                <font color="firebrick"> Free-form and region-based image editing made easy with language.</font>
            </div>
        </div>
        <br>

        <!-- NeurIPS-24-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/omnijarvis_arxiv24.png" alt="">
            </div>
            <div class="col-md-9">
                <a href="https://craftjarvis.org/">Team CraftJarvis <img src="img/craftjarvis1.png" alt="Icon" style="width: 100px; height: 25px; vertical-align: -5px;"></a>
                <br>
                <b>
                    <font color="black">OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents</font>
                </b><br>
                <a target="_blank" href="https://nips.cc/Conferences/2024"><b>NeurIPS 2024</b></font></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2407.00114">arXiv</a>&nbsp;/
                <a target="_blank" href="https://omnijarvis.github.io/">Project</a>&nbsp;/
                <a target="_blank" href="https://github.com/CraftJarvis/OmniJarvis">Code</a>&nbsp;
                <br>
                <font color="firebrick"> Native modeling of multimodal interaction (VLA) data. 
                </font>
            </div>
        </div>
        <br>

        <!-- ECCV-24-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/videoagent_arxiv24.png" alt="">
            </div>
            <div class="col-md-9">
                Yue Fan*,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma*</b></a>,
                Rujie Wu
                Yuntao Du,
                Jiaqi Li,
                Zhi Gao,
                <a href="https://liqing-ustc.github.io">Qing Li</a>
                <br>
                <b>
                    <font color="black">VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding</font>
                </b><br>
                <a href="https://eccv2024.ecva.net/"><b>ECCV 2024</b></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2403.11481">arXiv</a>&nbsp;/
                <a target="_blank" href="https://videoagent.github.io">Project</a>
                <br>
                <font color="firebrick"> Zero-shot long-form video understanding, shrinking the gap to Gemini.
                </font>
            </div>
        </div>
        <br>

        <!-- ECCV-24-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable fixed-height-img" src="img/pq3d_arxiv.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://zhuziyu-edward.github.io/'>Ziyu Zhu</a>,
                Zhuofan Zhang,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                <a href='https://nxsedson.github.io/'>Xuesong Niu</a>,
                <a href='https://yixchen.github.io/'>Yixin Chen</a>,
                <a href='https://buzz-beater.github.io'>Baoxiong Jia</a>,
                Zhidong Deng,
                <a href="https://siyuanhuang.com">Siyuan Huang</a>,
                <a href="https://liqing-ustc.github.io">Qing Li</a>
                <br>
                <b>
                    <font color="black">Unifying 3D Vision-Language Understanding via Promptable Queries</font>
                </b><br>
                <a href="https://eccv2024.ecva.net/"><b>ECCV 2024</b></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2405.11442">arXiv</a>&nbsp;/
                <a target="_blank" href="https://pq3d.github.io/">Project</a>&nbsp;/
                <a target="_blank" href="https://github.com/PQ3D/PQ3D">Code</a>&nbsp;
                <br>
                <font color="firebrick"> Unifying open-vocabulary perception and reasoning in 3D world.</font>
            </div>
        </div>
        <br>

        <!-- ICML-24-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/leo_arxiv23.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma*</b></a>,
                Jiangyong Huang*,
                Silong Yong*,
                Xiongkun Linghu*,
                Puhao Li,
                Yan Wang,
                <a href="https://liqing-ustc.github.io">Qing Li</a>,
                <a href="http://www.stat.ucla.edu/~sczhu">Song-Chun Zhu</a>,
                <a href='https://buzz-beater.github.io'>Baoxiong Jia</a>,
                <a href="https://siyuanhuang.com">Siyuan Huang</a>
                <br>
                <b>
                    <font color="black">LEO: An Embodied Generalist Agent in 3D World</font>
                </b><br>
                <a href="https://icml.cc/Conferences/2024"><b>ICML 2024</b></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2311.12871">arXiv</a>&nbsp;/
                <a target="_blank" href="https://embodied-generalist.github.io/">Project</a>&nbsp;/
                <a target="_blank" href="https://github.com/embodied-generalist/embodied-generalist">Code</a>&nbsp;/
                <a target="_blank" href="https://huggingface.co/spaces/embodied-generalist/LEO-Demo">Demo</a>&nbsp;/
                <a target="_blank" href="https://youtu.be/mlnjz4eSjB4?si=NN9z7TpkTPgBAzBw">Video</a>&nbsp;
            </div>
        </div>
        <br>

        <!-- NAACL-24-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/mindagent_arxiv23.png" alt="">
            </div>
            <div class="col-md-9">
                Ran Gong*,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma*</b></a>,
                Qiuyuan Huang*,
                Hoi Vo,
                Zane Durante
                Yusuke Noda,
                <a href="https://zilongzheng.github.io">Zilong Zheng</a>,
                <a href="http://www.stat.ucla.edu/~sczhu">Song-Chun Zhu</a>
                Demetri Terzopoulos,
                Li Fei-Fei,
                Jianfeng Gao
                <br>
                <b>
                    <font color="black">MindAgent: Emergent Gaming Interaction</font>
                </b><br>
                <a href="https://2024.naacl.org/"><b>NAACL 2024 Findings</b></a>&nbsp;/
                <a href="https://arxiv.org/pdf/2309.09971">Paper</a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2309.09971">arXiv</a>&nbsp;/
                <a target="_blank" href="https://mindagent.github.io/">Project</a>&nbsp;/
                <a target="_blank" href="#">Code</a>
                <br>
                <font color="firebrick"> Benchmark and other infra for LLM + general multi-player gaming.</font>
            </div>
        </div><br>

        <!-- CVPR-24-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/clova_arxiv23.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://zhigao2017.github.io/'>Zhi Gao</a>,
                Yuntao Du,
                Xintong Zhang,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                <a href="https://scholar.google.com/citations?user=rfVLLfAAAAAJ&hl=en">Wenjuan Han</a>,
                <a href="http://www.stat.ucla.edu/~sczhu">Song-Chun Zhu</a>,
                <a href="https://liqing-ustc.github.io">Qing Li</a>,
                <br>
                <b>
                    <font color="black">CLOVA: A Closed-LOop Visual Assistant with Tool Usage and Update</font>
                </b><br>
                <a href="https://cvpr.thecvf.com/Conferences/2024/"><b>CVPR 2024</b></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2312.10908">arXiv</a>&nbsp;/
                <a target="_blank" href="https://clova-tool.github.io/">Project</a>
                <br>
                <font color="firebrick"> A self-improved language agent that sharpens its tools.
                </font>
            </div>
        </div>
        <br>

        <!-- ICLR-24-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/groot_arxiv23.png" alt="">
            </div>
            <div class="col-md-9">
                <a href="https://craftjarvis.org/">Team CraftJarvis <img src="img/craftjarvis1.png" alt="Icon" style="width: 100px; height: 25px; vertical-align: -5px;"></a>
                <br>
                <b>
                    <font color="black">GROOT: Learning to Follow Instructions by Watching Gameplay Videos</font>
                </b><br>
                <a href="https://iclr.cc/Conferences/2024"><b>ICLR 2024</b></a>&nbsp;/
                <a href="file/groot_iclr24.pdf">Paper</a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2310.08235/">arXiv</a>&nbsp;/
                <a target="_blank" href="https://craftjarvis-groot.github.io/">Project</a>&nbsp;/
                <a target="_blank" href="https://github.com/CraftJarvis/GROOT">Code</a>&nbsp;
                <br>
                <font color="firebrick"> Spotlight presentation. Closing the human-machine gap on instruction following, measured by Elo rating.
                </font>
            </div>
        </div>
        <br>

        <!-- ICLR-24-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/mmicl_arxiv23.png" alt="">
            </div>
            <div class="col-md-9">
                <a href="https://scholar.google.cz/citations?user=skIXywUAAAAJ&hl=zh-CN">Haozhe Zhao*</a>,
                Zefan Cai,
                Zhuzheng Si,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                Kaikai An,
                Liang Chen,
                Zixuan Liu,
                Sheng Wang,
                <a href="https://scholar.google.com/citations?user=rfVLLfAAAAAJ&hl=en">Wenjuan Han</a>,
                <a href="https://scholar.google.com.au/citations?user=LaKNyhQAAAAJ&hl=en">Baobao Chang</a>,
                <br>
                <b>
                    <font color="black">MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning
                    </font>
                </b><br>
                <a href="https://iclr.cc/Conferences/2024"><b>ICLR 2024</b></a>&nbsp;/
                <a href="https://arxiv.org/pdf/2309.07915">Paper</a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2309.07915">arXiv</a>&nbsp;/
                <a target="_blank" href="http://www.testmmicl.work/">Demo</a>&nbsp;/
                <a target="_blank" href="https://github.com/HaozheZhao/MIC">Code</a>
                <br>
                <font color="firebrick"> Ranked 1st on MMBench and MME in 08/2023.</font>
            </div>
        </div>
        <br>

        <!-- ICLR-24-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/bow_arxiv23.jpg" alt="">
            </div>
            <div class="col-md-9">
                Rujie Wu*
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma*</b></a>,
                <a href="https://liqing-ustc.github.io">Qing Li</a>,
                Wei Wang,
                Zhenliang Zhang,
                <a href="http://www.stat.ucla.edu/~sczhu">Song-Chun Zhu</a>
                Yizhou Wang
                <br>
                <b>
                    <font color="black">Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real
                        World</font>
                </b><br>
                <a href="https://iclr.cc/Conferences/2024"><b>ICLR 2024</b></a>&nbsp;/
                <a href="file/bongardopenworld_iclr24.pdf">Paper</a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2210.07474">arXiv</a>&nbsp;/
                <a target="_blank" href="https://joyjayng.github.io/Bongard-OpenWorld.github.io/">Project</a>&nbsp;/
                <a target="_blank" href="https://github.com/joyjayng/Bongard-OpenWorld">Code</a>&nbsp;
                <br>
                <font color="firebrick"> The third chapter of the Bongard trilogy, for the LM era (<a
                        href='https://arxiv.org/abs/2010.00763'><b>chapter 1</b></a>, <a
                        href='https://arxiv.org/abs/2205.13803'><b>chapter 2</b></a>)</font>
            </div>
        </div>
        <br>


        <!-- NeurIPS-23-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/deps_neurips23.png" alt="">
            </div>
            <div class="col-md-9">
                <a href="https://craftjarvis.org/">Team CraftJarvis <img src="img/craftjarvis1.png" alt="Icon" style="width: 100px; height: 25px; vertical-align: -5px;"></a>
                <br>
                <b>
                    <font color="black">Describe, Explain, Plan and Select: Interactive Planning with Large Language
                        Models Enables Open-World Multi-Task Agents</font>
                </b><br>
                <a target="_blank" href="https://nips.cc/Conferences/2023"><b>NeurIPS 2023</b></font></a>&nbsp;/
                <a href="file/deps_neurips23.pdf">Paper</a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2302.01560">arXiv</a>&nbsp;/
                <a target="_blank" href="#">Project</a>&nbsp;/
                <a target="_blank" href="https://github.com/CraftJarvis/MC-Planner">Code</a>&nbsp;
                <br>
                <font color="firebrick"> Best paper award, ICML-23 TEACH Workshop</font>
            </div>
        </div>
        <br>

        <!-- ICCV23-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/3dvista_iccv23.png" alt="">
            </div>
            <div class="col-md-9">
                Ziyu Zhu,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                <a href='https://yixchen.github.io/'>Yixin Chen</a>,
                Zhidong Deng,
                <a href="https://siyuanhuang.com">Siyuan Huang</a>,
                <a href="https://liqing-ustc.github.io">Qing Li</a>
                <br>
                <b>
                    <font color="black">3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment</font>
                </b><br>
                <a href="https://iccv2023.thecvf.com/"><b>ICCV 2023</b></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2308.04352">arXiv</a>&nbsp;/
                <a target="_blank" href="https://3d-vista.github.io/">Project</a>&nbsp;/
                <a target="_blank" href="https://github.com/3d-vista/3D-VisTA">Code</a>&nbsp;
                <br>
                <font color="firebrick"> A new 3D-Language foundation model</font>
            </div>
        </div>
        <br>

        <!-- CVPR23-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/minecraft1_arxiv23.png" alt="">
            </div>
            <div class="col-md-9">
                <a href="https://craftjarvis.org/">Team CraftJarvis <img src="img/craftjarvis1.png" alt="Icon" style="width: 100px; height: 25px; vertical-align: -5px;"></a>
                <br>
                <b>
                    <font color="black">Open-World Multi-Task Control Through Goal-Aware Representation Learning and
                        Adaptive Horizon Prediction</font>
                </b><br>
                <a href="https://cvpr2023.thecvf.com/"><b>CVPR 2023</b></a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2301.10034">arXiv</a>&nbsp;/
                <a target="_blank" href="#">Project</a>&nbsp;/
                <a target="_blank" href="https://github.com/CraftJarvis/MC-Controller">Code</a>&nbsp;
            </div>
        </div>
        <br>

        <!-- ICLR23-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/sqa3d_iclr23.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                Silong Yong,
                <a href="https://zilongzheng.github.io">Zilong Zheng</a>,
                <a href="https://liqing-ustc.github.io">Qing Li</a>,
                <a href="https://web.cs.ucla.edu/~yliang/">Yitao Liang</a>,
                <a href="http://www.stat.ucla.edu/~sczhu">Song-Chun Zhu</a>,
                <a href="https://siyuanhuang.com">Siyuan Huang</a>
                <br>
                <b>
                    <font color="black">SQA3D: Situated Question Answering in 3D Scenes</font>
                </b><br>
                <a target="_blank" href="https://iclr.cc/Conferences/2023"><b>ICLR 2023</b></font></a>&nbsp;/
                <a href="file/sqa3d_iclr23.pdf">Paper</a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2210.07474">arXiv</a>&nbsp;/
                <a target="_blank" href="file/sqa3d_iclr23_slides.pdf">Slides</a>&nbsp;/
                <a target="_blank" href="https://sqa3d.github.io/">Project</a>&nbsp;/
                <a target="_blank" href="https://github.com/SilongYong/SQA3D">Code</a>&nbsp;/
                <a target="_blank" href="https://paperswithcode.com/dataset/sqa3d">Benchmark</a>&nbsp;
                <br>
                <font color="firebrick"> A new quest: embodied scene understanding</font>
            </div>
        </div>
        <br>

        <!-- ICML22-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/ldebm_icml22.png" alt="">
            </div>
            <div class="col-md-9">
                Peyi Yu,
                <a href='http://siruixie.com'>Sirui Xie</a>,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                <a href='https://buzz-beater.github.io'>Baoxiong Jia</a>,
                <a href='#'>Bo Pang</a>,
                <a href='https://ruiqigao.github.io/'>Ruiqi Gao</a>,
                <a href="https://yzhu.io">Yixin Zhu</a>,
                <a href="http://www.stat.ucla.edu/~sczhu">Song-Chun Zhu</a> and
                <a href="http://www.stat.ucla.edu/~ywu">Ying Nian Wu</a>
                <br>
                <b>
                    <font color="black">Latent Diffusion Energy-Based Model for Interpretable Text Modeling</font>
                </b><br>
                <a target="_blank" href="https://icml.cc/Conferences/2022"><b>ICML 2022</b></font></a>&nbsp;/
                <a target="_blank" href="file/ldebm_icml22.pdf">Paper</a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2206.05895">arXiv</a>&nbsp;/
                <a target="_blank" href="https://github.com/yuPeiyu98/LDEBM">Code</a>&nbsp;
                <br>
            </div>
        </div><br>

        <!-- CVPR22-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/bongardhoi_cvpr22.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://jianghz.me/'>Huaizu Jiang*</a>,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma*</b></a>,
                <a href='https://weilinie.github.io/'>Weili Nie</a>,
                <a href='https://chrisding.github.io/'>Zhiding Yu</a>,
                <a href='https://www.cs.utexas.edu/~yukez/'>Yuke Zhu</a>,
                <a href="http://www.stat.ucla.edu/~sczhu">Song-Chun Zhu</a>,
                <a href='http://tensorlab.cms.caltech.edu/users/anima/'>Anima Anandkumar</a>
                <br>
                <b>
                    <font color="black">Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object
                        Interactions</font>
                </b><br>
                <a target="_blank" href="https://cvpr2022.thecvf.com/"><b>CVPR 2022</b></font></a>&nbsp;/
                <a target="_blank" href="file/bongardhoi_cvpr22.pdf">Paper</a>&nbsp;/
                <a target="_blank" href="file/bongardhoi_cvpr22_poster.pdf">Poster</a>&nbsp;/
                <a target="_blank" href="file/bongardhoi_cvpr22_slides.pdf">Slides</a>&nbsp;/
                <a target="_blank" href="https://github.com/NVlabs/Bongard-HOI/blob/master/README.md">Project</a>&nbsp;/
                <a target="_blank" href="http://arxiv.org/abs/2205.13803">arXiv</a>&nbsp;/
                <a target="_blank" href="https://github.com/NVlabs/Bongard-HOI">Code</a>&nbsp;/
                <a target="_blank" href="file/bibtex/bongardhoi_cvpr22.bib">Bibtex</a>&nbsp;
                <br>
                <font color="firebrick"> Oral presentation</font>
            </div>
        </div><br>

        <!-- ICLR22-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/relvit_iclr22.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                <a href='https://weilinie.github.io/'>Weili Nie</a>,
                <a href='https://chrisding.github.io/'>Zhiding Yu</a>,
                <a href='https://jianghz.me/'>Huaizu Jiang</a>,
                <a href='https://xiaocw11.github.io/'>Chaowei Xiao</a>,
                <a href='https://www.cs.utexas.edu/~yukez/'>Yuke Zhu</a>,
                <a href="http://www.stat.ucla.edu/~sczhu">Song-Chun Zhu</a>,
                <a href='http://tensorlab.cms.caltech.edu/users/anima/'>Anima Anandkumar</a>
                <br>
                <b>
                    <font color="black">RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning</font>
                </b><br>
                <a target="_blank" href="https://iclr.cc/Conferences/2022"><b>ICLR 2022</b></font></a>&nbsp;/
                <a target="_blank" href="file/relvit_iclr22.pdf">Paper</a>&nbsp;/
                <a target="_blank" href="file/relvit_iclr22_poster.pdf">Poster</a>&nbsp;/
                <a target="_blank" href="file/relvit_iclr22_slides.pdf">Slides</a>&nbsp;/
                <a target="_blank" href="https://github.com/NVlabs/RelViT/blob/master/README.md">Project</a>&nbsp;/
                <a target="_blank" href="https://openreview.net/forum?id=afoV8W3-IYp">OpenReview</a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2204.11167">arXiv</a>&nbsp;/
                <a target="_blank" href="https://github.com/NVlabs/RelViT">Code</a>&nbsp;/
                <a target="_blank" href="file/bibtex/relvit_iclr22.bib">Bibtex</a>&nbsp;
                <br>
            </div>
        </div><br>


        <!-- NeurIPS21-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/drc_neurips21.png" alt="">
            </div>
            <div class="col-md-9">
                Peyi Yu,
                <a href='http://siruixie.com'>Sirui Xie</a>,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                <a href="https://yzhu.io">Yixin Zhu</a>,
                <a href="http://www.stat.ucla.edu/~ywu">Ying Nian Wu</a> and
                <a href="http://www.stat.ucla.edu/~sczhu">Song-Chun Zhu</a>
                <br>
                <b>
                    <font color="black">Unsupervised Foreground Extraction via Deep Region Competition</font>
                </b><br>
                <a target="_blank" href="https://nips.cc/Conferences/2021"><b>NeurIPS 2021</b></font></a>&nbsp;/
                <a target="_blank" href="file/drc_neurips21.pdf">Paper</a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2110.15497">arXiv</a>&nbsp;/
                <a target="_blank" href="https://github.com/yuPeiyu98/DRC">Code</a>&nbsp;
                <br>
            </div>
        </div><br>

        <!-- ICML21-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/optiongail_icml21.png" alt="">
            </div>
            <div class="col-md-9">
                Mingxuan Jing,
                <a href="https://sites.google.com/site/wenbinghuangshomepage/home">Wenbing Huang</a>,
                Fuchun Sun,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                <a href="http://taokong.github.io/">Tao Kong</a>,
                <a href="http://people.csail.mit.edu/ganchuang/">Chuang Gan</a> and
                <a href="https://sites.cs.ucsb.edu/~lilei/">Lei Li</a>
                <br>
                <b>
                    <font color="black">Adversarial Option-Aware Hierarchical Imitation Learning</font>
                </b><br>
                <a target="_blank" href="https://icml.cc/Conferences/2021"><b>ICML 2021</b></font></a>&nbsp;/
                <a target="_blank" href="file/optiongail_icml21.pdf">Paper</a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/2106.05530">arXiv</a>&nbsp;/
                <a target="_blank" href="https://github.com/id9502/Option-GAIL">Code</a>&nbsp;
                <br>
                <font color="firebrick"> Spotlight presentation</font>
            </div>
        </div><br>

        <!-- IROS20b-->
        <!-- <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/multimodal_pouring_iros20.png" alt="">
            </div>
            <div class="col-md-9">
                <a href="https://tams.informatik.uni-hamburg.de/people/liang">Hongzhuo Liang</a>,
                Chuangchuang Zhou,
                <a href="https://tams.informatik.uni-hamburg.de/people/sli">Shuang Li</a>,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                <a href="https://tams.informatik.uni-hamburg.de/people/hendrich">Norman Hendrich</a>,
                <a href="https://www.inf.uni-hamburg.de/en/inst/ab/sp/people/gerkmann.html">Timo Gerkmann</a>,
                Fuchun Sun and
                <a href="https://tams.informatik.uni-hamburg.de/people/zhang">Jianwei Zhang</a>
                <br>
                <b><font color="black">Robust Robotic Pouring using Audition and Haptics</font></b><br>
                <a target="_blank" href="https://iros2020.org"><b>IROS 2020</b></font></a>&nbsp;/
                    Paper&nbsp;/
                    <a target="_blank" href="https://lianghongzhuo.github.io/MultimodalPouring/">Project Page</a>&nbsp;/
                    <a target="_blank" href="https://arxiv.org/abs/2003.00342">arXiv</a>&nbsp;/
                    <a target="_blank" href="https://github.com/lianghongzhuo/MultimodalPouring">Code</a>&nbsp;/
                    <a target="_blank" href="https://www.youtube.com/watch?v=_U7zTyS338I">Video</a>&nbsp;
                <br>
                <font color="firebrick"> Oral presentation</font>
            </div>
        </div><br> -->

        <!-- IROS20a-->
        <!-- <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/multimodal_teleop_iros20.png" alt="">
            </div>
            <div class="col-md-9">
                <a href="https://tams.informatik.uni-hamburg.de/people/sli">Shuang Li</a>,
                Jiaxi Jiang,
                <a href="https://tams.informatik.uni-hamburg.de/people/ruppel">Philipp Ruppel</a>,
                <a href="https://tams.informatik.uni-hamburg.de/people/liang">Hongzhuo Liang</a>,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                <a href="https://tams.informatik.uni-hamburg.de/people/hendrich">Norman Hendrich</a>,
                Fuchun Sun and
                <a href="https://tams.informatik.uni-hamburg.de/people/zhang">Jianwei Zhang</a>
                <br>
                <b><font color="black">A Mobile Robot Hand-Arm Teleoperation System by Vision and IMU</font></b><br>
                <a target="_blank" href="https://iros2020.org"><b>IROS 2020</b></font></a>&nbsp;/
                    Paper&nbsp;/
                    <a target="_blank" href="https://smilels.github.io/multimodal-translation-teleop/">Project Page</a>&nbsp;/
                    <a target="_blank" href="https://arxiv.org/abs/2003.05212">arXiv</a>&nbsp;/
                    <a target="_blank" href="https://github.com/Smilels/multimodal-translation-teleop">Code</a>&nbsp;/
                    <a target="_blank" href="https://www.youtube.com/watch?v=rAj2IWl2ezs">Video</a>&nbsp;
                <br>
                <font color="firebrick"> Oral presentation</font>
            </div>
        </div><br> -->

        <!-- AAAI20a-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/openlock_aaai20.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://mjedmonds.com'>Mark Edmonds</a>,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                <a href='http://web.cs.ucla.edu/~syqi'>Siyuan Qi</a>,
                <a href="https://yzhu.io">Yixin Zhu</a>,
                <a href="http://cvl.psych.ucla.edu/">Hongjing Lu</a> and
                <a href="http://www.stat.ucla.edu/~sczhu">Song-Chun Zhu</a>
                <br>
                <b>
                    <font color="black">Theory-based Causal Transfer: Integrating Instance-level Induction and
                        Abstract-level Structure Learning</font>
                </b><br>
                <a target="_blank" href="https://aaai.org/Conferences/AAAI-20/"><b>AAAI 2020</b></font></a>&nbsp;/
                <a target="_blank" href="file/openlock_aaai20.pdf">Paper</a>&nbsp;/
                <a target="_blank" href="https://mjedmonds.com/projects/OpenLock/AAAI20_OpenLockLearner.html">Project
                    Page</a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/1911.11185">arXiv</a>&nbsp;/
                <a target="_blank" href="https://github.com/mjedmonds/OpenLockLearner-AAAI20">Code</a>&nbsp;
                <br>
                <font color="firebrick"> Oral presentation</font>
            </div>
        </div><br>

        <!-- AAAI20b-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/mcpo_aaai20.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>*,
                Mingxuan Jing*,
                <a href="https://sites.google.com/site/wenbinghuangshomepage/home">Wenbing Huang</a>,
                Fuchun Sun,
                Bin Fang and
                <a href="https://sites.google.com/site/thuliuhuaping/home">Huaping Liu</a>
                <br>
                <b>
                    <font color="black">Reinforcement Learning from Imperfect Demonstrations under Soft Expert Guidance
                    </font>
                </b><br>
                <a target="_blank" href="https://aaai.org/Conferences/AAAI-20/"><b>AAAI 2020</b></font></a>&nbsp;/
                <a target="_blank" href="file/mcpo_aaai20.pdf">Paper</a>&nbsp;/
                Project Page&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/1911.07109">arXiv</a>&nbsp;/
                Code&nbsp;
                <br>
                also in <a target="_blank" href="http://spirl.info/2019">Structure & Priors in Reinforcement Learning
                    Workshop @ <b>ICLR 2019</b></a><br>
            </div>
        </div><br>

        <!-- NeurIPS19-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/maxentlfo_neurips19.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>*,
                Chao Yang*,
                <a href="https://sites.google.com/site/wenbinghuangshomepage/home">Wenbing Huang</a>*,
                Fuchun Sun,
                <a href="https://sites.google.com/site/thuliuhuaping/home">Huaping Liu</a>,
                <a href="http://ranger.uta.edu/~huang/">Junzhou Huang</a> and
                <a href="http://people.csail.mit.edu/ganchuang/">Chuang Gan</a>,
                <br>
                <b>
                    <font color="black">Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement
                    </font>
                </b><br>
                <a target="_blank" href="https://nips.cc/Conferences/2019"><b>NeurIPS 2019</b></font></a>&nbsp;/
                <a target="_blank" href="file/maxentlfo_neurips19.pdf">Paper</a>&nbsp;/
                Project Page&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/1910.04417">arXiv</a>&nbsp;/
                Code&nbsp;
                <br>
                <font color="firebrick"> Spotlight presentation</font>
            </div>
        </div><br>

        <!-- IROS19-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/pouringnet_iros19.png" alt="">
            </div>
            <div class="col-md-9">
                <a href="https://tams.informatik.uni-hamburg.de/people/liang">Hongzhuo Liang</a>,
                <a href="https://tams.informatik.uni-hamburg.de/people/sli">Shuang Li</a>,
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>,
                <a href="https://tams.informatik.uni-hamburg.de/people/hendrich">Norman Hendrich</a>,
                <a href="https://www.inf.uni-hamburg.de/en/inst/ab/sp/people/gerkmann.html">Timo Gerkmann</a>,
                Fuchun Sun and
                <a href="https://tams.informatik.uni-hamburg.de/people/zhang">Jianwei Zhang</a>
                <br>
                <b>
                    <font color="black">Making Sense of Audio Vibration for Liquid Height Estimation in Robotic Pouring
                    </font>
                </b><br>
                <a target="_blank" href="https://iros2019.org"><b>IROS 2019</b></font></a>&nbsp;/
                <a target="_blank" href="file/pouringnet_iros19.pdf">Paper</a>&nbsp;/
                <a target="_blakc" href="https://lianghongzhuo.github.io/AudioPouring/">Project Page</a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/1910.04417">arXiv</a>&nbsp;/
                <a target="_blank" href="https://github.com/lianghongzhuo/AudioPouring">Code</a>&nbsp;/
                <a target="_blank" href="https://www.youtube.com/watch?v=Za8dDjGFE1k">Video</a>&nbsp;
            </div>
        </div><br>

        <!-- ICRA19a-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/pointnetgpd_icra19.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>*,
                <a href="https://tams.informatik.uni-hamburg.de/people/liang">Hongzhuo Liang</a>*,
                <a href="https://tams.informatik.uni-hamburg.de/people/sli">Shuang Li</a>,
                <a href="https://tams.informatik.uni-hamburg.de/people/goerner">Michael G√∂rner</a>,
                Song Tang,
                Bin Fang,
                Fuchun Sun and
                <a href="https://tams.informatik.uni-hamburg.de/people/zhang">Jianwei Zhang</a>
                <br>
                <b>
                    <font color="black">PointNetGPD: Detecting Grasp Configurations from Point Sets</font>
                </b><br>
                <a target="_blank" href="https://icra2019.org"><b>ICRA 2019</b></font></a>&nbsp;/
                <a target="_blank" href="file/pointnetgpd_icra19.pdf">Paper</a>&nbsp;/
                <a target="_blakc" href="https://lianghongzhuo.github.io/PointNetGPD/">Project Page</a>&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/1809.06267">arXiv</a>&nbsp;/
                <a target="_blank" href="https://github.com/lianghongzhuo/PointNetGPD">Code</a>&nbsp;/
                <a target="_blank" href="https://www.youtube.com/watch?v=RBFFCLiWhRw">Video</a>&nbsp;
            </div>
        </div><br>

        <!-- ICRA19b-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/teachnet_icra19.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>*,
                <a href="https://tams.informatik.uni-hamburg.de/people/sli">Shuang Li</a>*,
                <a href="https://tams.informatik.uni-hamburg.de/people/liang">Hongzhuo Liang</a>,
                <a href="https://tams.informatik.uni-hamburg.de/people/goerner">Michael G√∂rner</a>,
                <a href="https://tams.informatik.uni-hamburg.de/people/ruppel">Philipp Ruppel</a>,
                Bin Fang,
                Fuchun Sun and
                <a href="https://tams.informatik.uni-hamburg.de/people/zhang">Jianwei Zhang</a>
                <br>
                <b>
                    <font color="black">Vision-based Teleoperation of Shadow Dexterous Hand using End-to-End Deep Neural
                        Network</font>
                </b><br>
                <a target="_blank" href="https://icra2019.org"><b>ICRA 2019</b></font></a>&nbsp;/
                <a target="_blank" href="file/teachnet_icra19.pdf">Paper</a>&nbsp;/
                Project Page&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/1809.06268">arXiv</a>&nbsp;/
                <a target="_blank" href="https://github.com/Smilels/TeachNet_Teleoperation">Code</a>&nbsp;/
                <a target="_blank" href="https://www.youtube.com/watch?v=I1FTJ87CtDs">Video</a>&nbsp;
            </div>
        </div><br>

        <!-- AAAI19-->
        <div class="row">
            <div class="col-md-3">
                <img class="img-fluid img-thumbnail img-rounded zoomable" src="img/pbganirl_aaai19.png" alt="">
            </div>
            <div class="col-md-9">
                <a href='https://jeasinema.github.io'><b>Xiaojian Ma</b></a>*,
                Mingxuan Jing*,
                <a href="https://sites.google.com/site/wenbinghuangshomepage/home">Wenbing Huang</a>,
                Fuchun Sun and
                <a href="https://sites.google.com/site/thuliuhuaping/home">Huaping Liu</a>
                <br>
                <b>
                    <font color="black">Task Transfer by Preference-Based Cost Learning</font>
                </b><br>
                <a target="_blank" href="https://aaai.org/Conferences/AAAI-19/"><b>AAAI 2019</b></font></a>&nbsp;/
                <a target="_blank" href="file/pbganirl_aaai19.pdf">Paper</a>&nbsp;/
                Project Page&nbsp;/
                <a target="_blank" href="https://arxiv.org/abs/1805.04686">arXiv</a>&nbsp;/
                Code&nbsp;
                <br>
                <font color="firebrick"> Spotlight presentation</font>
            </div>
        </div><br>

    </div>

    <!-- Experience -->
    <div class="container">
        <h3 id="Experience" style="padding-top: 80px; margin-top: -80px;">Experience</h3>
        <hr>
        <ul>
            <li>
                2022.9 - 2023.1, Research Scientist Intern, DeepMind<br>
                Mentors: <a href="https://korymathewson.com/">Kory Mathewson</a>, <a
                    href="https://scholar.google.co.uk/citations?user=W_BEUq8AAAAJ&hl=en">Peter Humphreys</a>, Owen He,
                <a href="https://scholar.google.co.uk/citations?user=evIkDWoAAAAJ&hl=en">Adam Santoro</a>, <a
                    href="https://contrastiveconvergence.net">Timothy Lillicrap</a> and <a
                    href="https://www.cs.mcgill.ca/~dprecup/">Doina Precup</a>.
            </li>

            <li>
                2021.6 - 2021.12, Research Intern, NVIDIA Research<br>
                Mentors: <a href="https://weilinie.github.io/">Weili Nie</a>, <a href="https://jianghz.me/">Huaizu
                    Jiang</a>, <a href="https://xiaocw11.github.io/">Chaowei Xiao</a>, <a
                    href="https://chrisding.github.io/">Zhiding Yu</a>, <a href="https://www.cs.utexas.edu/~yukez/">Yuke
                    Zhu</a> and <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>.
            </li>
            <li>
                2020.6 - 2021.5, Research Intern, Google Brain Robotics<br>
                Mentors: <a href="https://scholar.google.com/citations?user=GuU6oA4AAAAJ&hl=en">Pannag Sanketi</a> and
                <a href="https://scholar.google.com/citations?user=0NHagNQAAAAJ&hl=en">Laura Graesser</a>.
            </li>
            <li>
                2019.6 - 2019.9, Research Intern, ByteDance AI Lab<br>
                Mentors: <a href="http://taokong.github.io/">Tao Kong</a> and <a
                    href="https://sites.cs.ucsb.edu/~lilei/">Lei Li</a>.
            </li>
            <!--
            <li>
                2018.7 - 2018.11, Research Intern, University of California, Los Angeles, Center for Vision, Cognition, Learning and Autonomy<br>
                Advisors: <a href="http://www.stat.ucla.edu/~sczhu/" target="_blank">Song-Chun Zhu</a>, <a href="https://yzhu.io">Yixin Zhu</a> and <a href="https://mjedmonds.com">Mark Edmonds</a>.
            </li>
            -->
            <li>
                2017.7 - 2017.9, Research Intern, CVRP Lab, School of Computer Science, National University of
                Singapore<br>
                Mentor: <a href="https://www.comp.nus.edu.sg/~leegh" target="_blank">Gim Hee Lee</a>.
            </li>
        </ul>
    </div>

    <!--
    <div class="container">
        <h3 id="Teaching" style="padding-top: 80px; margin-top: -80px;">Teaching</h3><hr>
        <ul>
            <li><p><b>STATS 115: Probabilistic Decision Making</b>, UCLA (w/ Tao Gao), Winter 2020</p></li>
        </ul>
        <br>
    </div>
    -->

    <!-- Services -->
    <!--
    <div class="container">
        <h3 id="Service" style="padding-top: 80px; margin-top: -80px;">Professional Service</h3>
        <hr>
        <ul>
            <li><b>Conference PC Member/Reviewer</b>: ICML (top reviewer), NeurIPS, ICLR, AAAI, IJCAI (senior PC), CVPR
                (outstanding reviewer), ICCV, ECCV, WACV, ACCV, RSS, ICRA, IROS, CoRL</li>
            <li><b>Journal Reviewer</b>: <a href="https://www.jmlr.org/tmlr/">TMLR</a>, <a
                    href="https://www.ieee-ras.org/publications/ra-l">IEEE RA-L</a>, <a
                    href="http://www.aas.net.cn/CN/volumn/current.shtml">AAS</a>, <a
                    href="http://www.ieee-jas.org/">IEEE/CAA JAS</a></li>
        </ul>
        <br>
    </div>
    -->

    <!-- Contact -->
    <div class="container">
        <h3 id="Contact" style="padding-top: 80px; margin-top: -80px;">Contact</h3>
        jeasinema [at] gmail [dot] com <br>
        <a href="https://scholar.google.com/citations?user=xZec9goAAAAJ">[<span style="color:#0000ff">G</span><span
                style="color:#ff0000">o</span><span style="color:#ffd700">o</span><span
                style="color:#0000ff">g</span><span style="color:#008000">l</span><span style="color:#ff0000">e</span>
            Scholar]</a> &nbsp;| &nbsp;<a href="https://github.com/jeasinema">[GitHub]</a> &nbsp;
        <br>
    </div>

    <!-- Footer -->
    <div class="container">
        <br>
        <hr>
        <div class="row">
            <div class="col-md-9">
                <p align="left">&copy; Xiaojian Ma 2024</p>
            </div>
            <div class="col-md-3">
                <script type="text/javascript" id="clustrmaps"
                    src="//cdn.clustrmaps.com/map_v2.js?d=SOao3eeEOxmGJm2zvBirZBmhploRlKP0aTkiG515ji4&cl=ffffff&w=a"></script>
            </div>
        </div>
    </div>
    <!-- /container -->

    <!-- Bootstrap core JavaScript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
        integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"
        integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js"
        integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1"
        crossorigin="anonymous"></script>
</body>
<link href="file/custom.css" rel="stylesheet">

<!-- Custom Script to Create Modals Dynamically -->
<script>
    document.querySelectorAll('.zoomable').forEach((img, index) => {
        img.addEventListener('click', function() {
            const modalId = 'imageModal' + index;
            let modal = document.getElementById(modalId);

            if (!modal) {
                modal = document.createElement('div');
                modal.className = 'modal fade';
                modal.id = modalId;
                modal.tabIndex = -1;
                modal.role = 'dialog';
                modal.ariaLabelledby = 'imageModalLabel';
                modal.ariaHidden = true;
                modal.innerHTML = `
                    <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                        <div class="modal-content">
                            <div class="modal-body text-center">
                                <img class="img-fluid" src="${img.src}" alt="">
                            </div>
                        </div>
                    </div>
                `;
                document.body.appendChild(modal);
            }

            $(modal).modal('show');
        });
    });
</script>

</html>
